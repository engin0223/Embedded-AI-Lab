{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "arch.json  docker_run.sh  dpu.xclbin\t\tPROMPT.txt   Quantize.ipynb\n",
      "build\t   dpu.bit\t  local_utils.py\t__pycache__  weights.pth\n",
      "data\t   dpu.hwh\t  notebook_train.ipynb\tquant_dir\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZL5HI3Rj3blg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import local_utils\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Instantiate MiniResNet.\n",
    "\n",
    "Apply evaluation mode (method `.eval()`) to prevent of batch normalization layers parameters changes.\n",
    "\n",
    "Load state dict with mapping location to cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H1qAmZ7D3jMx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your model\n",
    "# apply eval() method\n",
    "net = ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Instantiate train and test loaders with batch size = 1.\n",
    "\n",
    "Extract 5% of training data (iterate in for loop to get a random samples).\n",
    "\n",
    "Collect data and labels as lists.\n",
    "\n",
    "Concatenate both lists (separately).\n",
    "\n",
    "Initialize LoaderWrapper with results of concatenation and batch size = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L4tpsFQX4jip"
   },
   "outputs": [],
   "source": [
    "# concatenation: torch.cat()\n",
    "\n",
    "\n",
    "class LoaderWrapper:\n",
    "    def __init__(self, data, labels, batch_size=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if index >= len(self):\n",
    "            raise StopIteration()\n",
    "\n",
    "        beg = index*self.batch_size\n",
    "        end = beg+self.batch_size\n",
    "        return self.data[beg:end], self.labels[beg:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.batch_size\n",
    "\n",
    "\n",
    "train_loader = ...          \n",
    "test_loader = ...\n",
    "\n",
    "quantizaton_data = []      \n",
    "quantizaton_labels = []      \n",
    "\n",
    "...\n",
    "quantization_loader = ...\n",
    "\n",
    "del train_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Instantiate accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "outputId": "3aea7dc1-a5e2-4f08-e7e2-f2af247666e9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             evaluator\n",
    "             ):\n",
    "    \"\"\"\n",
    "    :param model: torch.nn.Model\n",
    "    :param dataloader: data generator / loader\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    with torch.no_grad(), tm:\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i, XY in enumerate(dataloader):\n",
    "            X = XY[0]\n",
    "            Y = XY[1:]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, *Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "    print(tm)\n",
    "\n",
    "\n",
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             evaluator):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Quantization\", len(dataloader))\n",
    "    with tm:\n",
    "        # available in docker or after packaging \n",
    "        # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "        # and installing the package\n",
    "        from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "        # model to device\n",
    "        model = float_model.to(device)\n",
    "\n",
    "        # That was present in vai tutorial.\n",
    "        # I don't know if it affects to anything?\n",
    "        # Force to merge BN with CONV for better quantization accuracy\n",
    "        optimize = 1\n",
    "\n",
    "        rand_in = torch.randn(input_shape)\n",
    "        print(\"get qunatizer start\")\n",
    "        try:\n",
    "            quantizer = torch_quantizer(\n",
    "                quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"exception:\")\n",
    "            print(e)\n",
    "            return\n",
    "        print(\"get qunatizer end\")\n",
    "\n",
    "        print(\"get quantized model start\")\n",
    "        quantized_model = quantizer.quant_model\n",
    "        print(\"get quantized model end\")\n",
    "\n",
    "        # evaluate\n",
    "        print(\"testing st\")\n",
    "        evaluate(quantized_model, dataloader, evaluator)\n",
    "        print(\"testing end\")\n",
    "\n",
    "        # export config\n",
    "        if quant_mode == 'calib':\n",
    "            print(\"export config\")\n",
    "            quantizer.export_quant_config()\n",
    "            print(\"export config end\")\n",
    "        # export model\n",
    "        if quant_mode == 'test':\n",
    "            print(\"export xmodel\")\n",
    "            quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "            print(\"export xmodel end\")\n",
    "    print(tm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Evaluate network floating-point model with  test loader and quantization loader with accuracy evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PeaKZrJc3blk",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 10000/10000. Score = 0.983299970626831\n",
      "Execution time: 0:0:28:0, processed 10000 frames, throughput: 357.14285714285717 fps.\n",
      "Evaluation 3001/3001. Score = 0.9866710901260376\n",
      "Execution time: 0:0:7:0, processed 3001 frames, throughput: 428.7142857142857 fps.\n"
     ]
    }
   ],
   "source": [
    "# You can evaluate your floating point model first \n",
    "evaluate(...)\n",
    "evaluate(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vitis AI Quantizer for Post Training Quantization uses two stages.\n",
    "\n",
    "First is `calib` mode (calibration) - VAI Quantizer parses the model and adjust quantization parameters.\n",
    "\n",
    "Second is evaluation / test mode - after this step \n",
    "\n",
    "(theoretically after check is there is not too much of accuracy loss) \n",
    "\n",
    "model is exported in onnx format.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Run quantization for network in 'calib' mode with input shape = [1, 1, 28, 28].\n",
    "\n",
    "Use quantization loader and accuracy metric evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UqCYtQlO3bll",
    "outputId": "7103e738-593a-4300-81ed-3acf6a7387e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing MiniResNet...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/MiniResNet.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 3001/3001. Score = 0.9876707792282104\n",
      "Execution time: 1:0:27:0, processed 3001 frames, throughput: 34.49425287356322 fps.\n",
      "testing end\n",
      "export config\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quant_dir/quant_info.json)\u001b[0m\n",
      "export config end\n",
      "Execution time: 1:0:28:0, processed 3001 frames, throughput: 34.10227272727273 fps.\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - calib - is slow\n",
    "quantize(\n",
    "         ...,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Run quantization in `test` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hW7mQGoL3blm",
    "outputId": "ffe3ef85-8078-4f53-87cc-62a3b036c36e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing MiniResNet...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/MiniResNet.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 3001/3001. Score = 0.9880039691925049\n",
      "Execution time: 0:0:17:0, processed 3001 frames, throughput: 176.52941176470588 fps.\n",
      "testing end\n",
      "export xmodel\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Successfully convert 'MiniResNet' to xmodel.(quant_dir/MiniResNet_int.xmodel)\u001b[0m\n",
      "export xmodel end\n",
      "Execution time: 0:0:17:0, processed 3001 frames, throughput: 176.52941176470588 fps.\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - test - is faster\n",
    "\n",
    "quantize(\n",
    "         ...,\n",
    "         quant_dir='quant_dir',  # directory for quantizer results\n",
    "         )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Compile the quantized model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5MApToNQ3bln",
    "outputId": "aa19a1c1-66a9-465f-cb22-43214b612ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2\n",
      "[UNILOG][INFO] Graph name: MiniResNet, with op num: 130\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/build/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/build/MiniResnet_VAI.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 600f23936dab1908a0a723a09efdafc7, and has been saved to \"/workspace/build/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "# --xmodel quant_dir+'/'+{python class model name}+'_int.xmodel' - the result of quantization\n",
    "# --arch file dpu fingerprint (denotes DPU architecture and supported operations) - *.json file \n",
    "# --net_name name of network  - any name\n",
    "# --output_dir directory where results will be stored\n",
    "!vai_c_xir --xmodel ... --arch arch.json --net_name ... --output_dir  build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ISD7IjSg3blp"
   },
   "source": [
    "13. Save this file. Close Jupyter server. Exit from Vitis AI docker environment (`exit` command)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
